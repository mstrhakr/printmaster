package storage

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"time"
)

// AggregatedMetrics represents aggregated metrics with min/max/avg values
type AggregatedMetrics struct {
	ID           int64                  `json:"id"`
	Serial       string                 `json:"serial"`
	BucketStart  time.Time              `json:"bucket_start"`
	SampleCount  int                    `json:"sample_count"`
	PageCountMin int                    `json:"page_count_min"`
	PageCountMax int                    `json:"page_count_max"`
	PageCountAvg int                    `json:"page_count_avg"`
	ColorMin     int                    `json:"color_pages_min"`
	ColorMax     int                    `json:"color_pages_max"`
	ColorAvg     int                    `json:"color_pages_avg"`
	MonoMin      int                    `json:"mono_pages_min"`
	MonoMax      int                    `json:"mono_pages_max"`
	MonoAvg      int                    `json:"mono_pages_avg"`
	ScanMin      int                    `json:"scan_count_min"`
	ScanMax      int                    `json:"scan_count_max"`
	ScanAvg      int                    `json:"scan_count_avg"`
	TonerAvg     map[string]interface{} `json:"toner_levels_avg"`
}

// DownsampleRawToHourly aggregates raw 5-minute metrics into hourly buckets
// Returns the number of buckets created and any error
func (s *SQLiteStore) DownsampleRawToHourly(ctx context.Context, olderThan time.Time) (int, error) {
	// Query raw metrics grouped by serial and hour
	query := `
		SELECT 
			serial,
			strftime('%Y-%m-%d %H:00:00', timestamp) as hour_start,
			COUNT(*) as sample_count,
			MIN(page_count) as page_count_min,
			MAX(page_count) as page_count_max,
			AVG(page_count) as page_count_avg,
			MIN(color_pages) as color_min,
			MAX(color_pages) as color_max,
			AVG(color_pages) as color_avg,
			MIN(mono_pages) as mono_min,
			MAX(mono_pages) as mono_max,
			AVG(mono_pages) as mono_avg,
			MIN(scan_count) as scan_min,
			MAX(scan_count) as scan_max,
			AVG(scan_count) as scan_avg,
			GROUP_CONCAT(toner_levels) as toner_samples
		FROM metrics_raw
		WHERE timestamp < ?
		GROUP BY serial, hour_start
		ORDER BY serial, hour_start
	`

	rows, err := s.db.QueryContext(ctx, query, olderThan)
	if err != nil {
		return 0, fmt.Errorf("failed to query raw metrics for hourly aggregation: %w", err)
	}
	defer rows.Close()

	count := 0
	for rows.Next() {
		var serial, hourStartStr, tonerSamplesStr string
		var sampleCount, pageMin, pageMax, pageAvg, colorMin, colorMax, colorAvg int
		var monoMin, monoMax, monoAvg, scanMin, scanMax, scanAvg int

		err := rows.Scan(
			&serial, &hourStartStr, &sampleCount,
			&pageMin, &pageMax, &pageAvg,
			&colorMin, &colorMax, &colorAvg,
			&monoMin, &monoMax, &monoAvg,
			&scanMin, &scanMax, &scanAvg,
			&tonerSamplesStr,
		)
		if err != nil {
			if storageLogger != nil {
				storageLogger.Error("Failed to scan hourly aggregate row", "error", err)
			}
			continue
		}

		// Parse hour_start timestamp
		hourStart, err := time.Parse("2006-01-02 15:04:05", hourStartStr)
		if err != nil {
			if storageLogger != nil {
				storageLogger.Error("Failed to parse hour_start", "hour_start", hourStartStr, "error", err)
			}
			continue
		}

		// Average toner levels across samples
		tonerAvg := averageTonerLevels(tonerSamplesStr)
		tonerJSON, _ := json.Marshal(tonerAvg)

		// Insert into metrics_hourly (using INSERT OR REPLACE for idempotency)
		insertQuery := `
			INSERT OR REPLACE INTO metrics_hourly (
				serial, hour_start, sample_count,
				page_count_min, page_count_max, page_count_avg,
				color_pages_min, color_pages_max, color_pages_avg,
				mono_pages_min, mono_pages_max, mono_pages_avg,
				scan_count_min, scan_count_max, scan_count_avg,
				toner_levels_avg
			) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
		`

		_, err = s.db.ExecContext(ctx, insertQuery,
			serial, hourStart, sampleCount,
			pageMin, pageMax, pageAvg,
			colorMin, colorMax, colorAvg,
			monoMin, monoMax, monoAvg,
			scanMin, scanMax, scanAvg,
			string(tonerJSON),
		)

		if err != nil {
			if storageLogger != nil {
				storageLogger.Error("Failed to insert hourly aggregate", "serial", serial, "hour", hourStartStr, "error", err)
			}
			continue
		}

		count++
	}

	if storageLogger != nil && count > 0 {
		storageLogger.Info("Downsampled raw metrics to hourly", "buckets_created", count, "older_than", olderThan)
	}

	return count, nil
}

// DownsampleHourlyToDaily aggregates hourly metrics into daily buckets
func (s *SQLiteStore) DownsampleHourlyToDaily(ctx context.Context, olderThan time.Time) (int, error) {
	query := `
		SELECT 
			serial,
			strftime('%Y-%m-%d 00:00:00', hour_start) as day_start,
			SUM(sample_count) as sample_count,
			MIN(page_count_min) as page_count_min,
			MAX(page_count_max) as page_count_max,
			AVG(page_count_avg) as page_count_avg,
			MIN(color_pages_min) as color_min,
			MAX(color_pages_max) as color_max,
			AVG(color_pages_avg) as color_avg,
			MIN(mono_pages_min) as mono_min,
			MAX(mono_pages_max) as mono_max,
			AVG(mono_pages_avg) as mono_avg,
			MIN(scan_count_min) as scan_min,
			MAX(scan_count_max) as scan_max,
			AVG(scan_count_avg) as scan_avg,
			GROUP_CONCAT(toner_levels_avg) as toner_samples
		FROM metrics_hourly
		WHERE hour_start < ?
		GROUP BY serial, day_start
		ORDER BY serial, day_start
	`

	rows, err := s.db.QueryContext(ctx, query, olderThan)
	if err != nil {
		return 0, fmt.Errorf("failed to query hourly metrics for daily aggregation: %w", err)
	}
	defer rows.Close()

	count := 0
	for rows.Next() {
		var serial, dayStartStr, tonerSamplesStr string
		var sampleCount, pageMin, pageMax, pageAvg, colorMin, colorMax, colorAvg int
		var monoMin, monoMax, monoAvg, scanMin, scanMax, scanAvg int

		err := rows.Scan(
			&serial, &dayStartStr, &sampleCount,
			&pageMin, &pageMax, &pageAvg,
			&colorMin, &colorMax, &colorAvg,
			&monoMin, &monoMax, &monoAvg,
			&scanMin, &scanMax, &scanAvg,
			&tonerSamplesStr,
		)
		if err != nil {
			if storageLogger != nil {
				storageLogger.Error("Failed to scan daily aggregate row", "error", err)
			}
			continue
		}

		dayStart, err := time.Parse("2006-01-02 15:04:05", dayStartStr)
		if err != nil {
			if storageLogger != nil {
				storageLogger.Error("Failed to parse day_start", "day_start", dayStartStr, "error", err)
			}
			continue
		}

		tonerAvg := averageTonerLevels(tonerSamplesStr)
		tonerJSON, _ := json.Marshal(tonerAvg)

		insertQuery := `
			INSERT OR REPLACE INTO metrics_daily (
				serial, day_start, sample_count,
				page_count_min, page_count_max, page_count_avg,
				color_pages_min, color_pages_max, color_pages_avg,
				mono_pages_min, mono_pages_max, mono_pages_avg,
				scan_count_min, scan_count_max, scan_count_avg,
				toner_levels_avg
			) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
		`

		_, err = s.db.ExecContext(ctx, insertQuery,
			serial, dayStart, sampleCount,
			pageMin, pageMax, pageAvg,
			colorMin, colorMax, colorAvg,
			monoMin, monoMax, monoAvg,
			scanMin, scanMax, scanAvg,
			string(tonerJSON),
		)

		if err != nil {
			if storageLogger != nil {
				storageLogger.Error("Failed to insert daily aggregate", "serial", serial, "day", dayStartStr, "error", err)
			}
			continue
		}

		count++
	}

	if storageLogger != nil && count > 0 {
		storageLogger.Info("Downsampled hourly metrics to daily", "buckets_created", count, "older_than", olderThan)
	}

	return count, nil
}

// DownsampleDailyToMonthly aggregates daily metrics into monthly buckets
func (s *SQLiteStore) DownsampleDailyToMonthly(ctx context.Context, olderThan time.Time) (int, error) {
	query := `
		SELECT 
			serial,
			strftime('%Y-%m-01 00:00:00', day_start) as month_start,
			SUM(sample_count) as sample_count,
			MIN(page_count_min) as page_count_min,
			MAX(page_count_max) as page_count_max,
			AVG(page_count_avg) as page_count_avg,
			MIN(color_pages_min) as color_min,
			MAX(color_pages_max) as color_max,
			AVG(color_pages_avg) as color_avg,
			MIN(mono_pages_min) as mono_min,
			MAX(mono_pages_max) as mono_max,
			AVG(mono_pages_avg) as mono_avg,
			MIN(scan_count_min) as scan_min,
			MAX(scan_count_max) as scan_max,
			AVG(scan_count_avg) as scan_avg,
			GROUP_CONCAT(toner_levels_avg) as toner_samples
		FROM metrics_daily
		WHERE day_start < ?
		GROUP BY serial, month_start
		ORDER BY serial, month_start
	`

	rows, err := s.db.QueryContext(ctx, query, olderThan)
	if err != nil {
		return 0, fmt.Errorf("failed to query daily metrics for monthly aggregation: %w", err)
	}
	defer rows.Close()

	count := 0
	for rows.Next() {
		var serial, monthStartStr, tonerSamplesStr string
		var sampleCount, pageMin, pageMax, pageAvg, colorMin, colorMax, colorAvg int
		var monoMin, monoMax, monoAvg, scanMin, scanMax, scanAvg int

		err := rows.Scan(
			&serial, &monthStartStr, &sampleCount,
			&pageMin, &pageMax, &pageAvg,
			&colorMin, &colorMax, &colorAvg,
			&monoMin, &monoMax, &monoAvg,
			&scanMin, &scanMax, &scanAvg,
			&tonerSamplesStr,
		)
		if err != nil {
			if storageLogger != nil {
				storageLogger.Error("Failed to scan monthly aggregate row", "error", err)
			}
			continue
		}

		monthStart, err := time.Parse("2006-01-02 15:04:05", monthStartStr)
		if err != nil {
			if storageLogger != nil {
				storageLogger.Error("Failed to parse month_start", "month_start", monthStartStr, "error", err)
			}
			continue
		}

		tonerAvg := averageTonerLevels(tonerSamplesStr)
		tonerJSON, _ := json.Marshal(tonerAvg)

		insertQuery := `
			INSERT OR REPLACE INTO metrics_monthly (
				serial, month_start, sample_count,
				page_count_min, page_count_max, page_count_avg,
				color_pages_min, color_pages_max, color_pages_avg,
				mono_pages_min, mono_pages_max, mono_pages_avg,
				scan_count_min, scan_count_max, scan_count_avg,
				toner_levels_avg
			) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
		`

		_, err = s.db.ExecContext(ctx, insertQuery,
			serial, monthStart, sampleCount,
			pageMin, pageMax, pageAvg,
			colorMin, colorMax, colorAvg,
			monoMin, monoMax, monoAvg,
			scanMin, scanMax, scanAvg,
			string(tonerJSON),
		)

		if err != nil {
			if storageLogger != nil {
				storageLogger.Error("Failed to insert monthly aggregate", "serial", serial, "month", monthStartStr, "error", err)
			}
			continue
		}

		count++
	}

	if storageLogger != nil && count > 0 {
		storageLogger.Info("Downsampled daily metrics to monthly", "buckets_created", count, "older_than", olderThan)
	}

	return count, nil
}

// CleanupOldTieredMetrics removes metrics from all tiers based on retention policies
// Returns a map with counts of deleted records per tier
func (s *SQLiteStore) CleanupOldTieredMetrics(ctx context.Context, rawRetentionDays, hourlyRetentionDays, dailyRetentionDays int) (map[string]int, error) {
	now := time.Now()
	results := make(map[string]int)

	// Cleanup raw metrics (default 7 days)
	rawCutoff := now.AddDate(0, 0, -rawRetentionDays)
	rawResult, err := s.db.ExecContext(ctx, "DELETE FROM metrics_raw WHERE timestamp < ?", rawCutoff)
	if err != nil {
		return results, fmt.Errorf("failed to cleanup raw metrics: %w", err)
	}
	rawCount, _ := rawResult.RowsAffected()
	results["raw"] = int(rawCount)

	// Cleanup hourly metrics (default 30 days)
	hourlyCutoff := now.AddDate(0, 0, -hourlyRetentionDays)
	hourlyResult, err := s.db.ExecContext(ctx, "DELETE FROM metrics_hourly WHERE hour_start < ?", hourlyCutoff)
	if err != nil {
		return results, fmt.Errorf("failed to cleanup hourly metrics: %w", err)
	}
	hourlyCount, _ := hourlyResult.RowsAffected()
	results["hourly"] = int(hourlyCount)

	// Cleanup daily metrics (default 365 days)
	dailyCutoff := now.AddDate(0, 0, -dailyRetentionDays)
	dailyResult, err := s.db.ExecContext(ctx, "DELETE FROM metrics_daily WHERE day_start < ?", dailyCutoff)
	if err != nil {
		return results, fmt.Errorf("failed to cleanup daily metrics: %w", err)
	}
	dailyCount, _ := dailyResult.RowsAffected()
	results["daily"] = int(dailyCount)

	// Monthly metrics are kept forever (no cleanup)
	results["monthly"] = 0

	if storageLogger != nil {
		storageLogger.Info("Cleaned up old tiered metrics",
			"raw_deleted", rawCount,
			"hourly_deleted", hourlyCount,
			"daily_deleted", dailyCount,
			"raw_retention_days", rawRetentionDays,
			"hourly_retention_days", hourlyRetentionDays,
			"daily_retention_days", dailyRetentionDays,
		)
	}

	return results, nil
}

// PerformFullDownsampling runs all downsampling operations in sequence
// This should be called periodically (e.g., every 6-12 hours)
func (s *SQLiteStore) PerformFullDownsampling(ctx context.Context) error {
	now := time.Now()

	// Downsample raw → hourly (process data older than 1 hour)
	hourAgo := now.Add(-1 * time.Hour)
	hourlyCount, err := s.DownsampleRawToHourly(ctx, hourAgo)
	if err != nil {
		return fmt.Errorf("failed to downsample raw to hourly: %w", err)
	}

	// Downsample hourly → daily (process data older than 1 day)
	dayAgo := now.AddDate(0, 0, -1)
	dailyCount, err := s.DownsampleHourlyToDaily(ctx, dayAgo)
	if err != nil {
		return fmt.Errorf("failed to downsample hourly to daily: %w", err)
	}

	// Downsample daily → monthly (process data older than 1 month)
	monthAgo := now.AddDate(0, -1, 0)
	monthlyCount, err := s.DownsampleDailyToMonthly(ctx, monthAgo)
	if err != nil {
		return fmt.Errorf("failed to downsample daily to monthly: %w", err)
	}

	// Cleanup old data from each tier
	cleanupResults, err := s.CleanupOldTieredMetrics(ctx, 7, 30, 365)
	if err != nil {
		return fmt.Errorf("failed to cleanup old metrics: %w", err)
	}

	if storageLogger != nil {
		storageLogger.Info("Full downsampling complete",
			"hourly_buckets", hourlyCount,
			"daily_buckets", dailyCount,
			"monthly_buckets", monthlyCount,
			"raw_deleted", cleanupResults["raw"],
			"hourly_deleted", cleanupResults["hourly"],
			"daily_deleted", cleanupResults["daily"],
		)
	}

	return nil
}

// averageTonerLevels takes a comma-separated list of JSON toner level objects
// and returns a map with averaged values
func averageTonerLevels(samplesStr string) map[string]interface{} {
	if samplesStr == "" {
		return make(map[string]interface{})
	}

	// Split by comma (from GROUP_CONCAT)
	samples := []string{}
	current := ""
	inQuotes := false
	braceCount := 0

	// Parse GROUP_CONCAT result which may contain JSON with commas
	for _, char := range samplesStr {
		if char == '"' {
			inQuotes = !inQuotes
		}
		if !inQuotes {
			switch char {
			case '{':
				braceCount++
			case '}':
				braceCount--
			}
		}

		if char == ',' && !inQuotes && braceCount == 0 {
			samples = append(samples, current)
			current = ""
		} else {
			current += string(char)
		}
	}
	if current != "" {
		samples = append(samples, current)
	}

	// Parse each JSON sample and accumulate values
	totals := make(map[string]float64)
	counts := make(map[string]int)

	for _, sample := range samples {
		var levels map[string]interface{}
		if err := json.Unmarshal([]byte(sample), &levels); err != nil {
			continue
		}

		for color, value := range levels {
			if v, ok := value.(float64); ok {
				totals[color] += v
				counts[color]++
			}
		}
	}

	// Calculate averages
	result := make(map[string]interface{})
	for color, total := range totals {
		if count := counts[color]; count > 0 {
			result[color] = int(total / float64(count))
		}
	}

	return result
}

// GetTieredMetricsHistory retrieves metrics from appropriate tiers based on time range
// This is a smarter version of GetMetricsHistory that queries the right tier
func (s *SQLiteStore) GetTieredMetricsHistory(ctx context.Context, serial string, since time.Time, until time.Time) ([]*MetricsSnapshot, error) {
	if serial == "" {
		return nil, ErrInvalidSerial
	}

	now := time.Now()
	sevenDaysAgo := now.AddDate(0, 0, -7)
	thirtyDaysAgo := now.AddDate(0, 0, -30)
	oneYearAgo := now.AddDate(0, 0, -365)

	var snapshots []*MetricsSnapshot

	// Determine which tiers to query based on time range
	needRaw := since.After(sevenDaysAgo) || until.After(sevenDaysAgo)
	needHourly := (since.Before(sevenDaysAgo) && since.After(thirtyDaysAgo)) || (until.Before(sevenDaysAgo) && until.After(thirtyDaysAgo))
	needDaily := (since.Before(thirtyDaysAgo) && since.After(oneYearAgo)) || (until.Before(thirtyDaysAgo) && until.After(oneYearAgo))
	needMonthly := since.Before(oneYearAgo) || until.Before(oneYearAgo)

	// Query raw metrics (last 7 days)
	if needRaw {
		rawQuery := `
			SELECT id, serial, timestamp, page_count, color_pages, mono_pages, scan_count, toner_levels
			FROM metrics_raw
			WHERE serial = ?
			ORDER BY timestamp ASC
		`
		rows, err := s.db.QueryContext(ctx, rawQuery, serial)
		if err != nil {
			return nil, fmt.Errorf("failed to query raw metrics: %w", err)
		}
		defer rows.Close()

		for rows.Next() {
			snapshot := &MetricsSnapshot{}
			var tonerJSON sql.NullString

			err := rows.Scan(
				&snapshot.ID, &snapshot.Serial, &snapshot.Timestamp,
				&snapshot.PageCount, &snapshot.ColorPages, &snapshot.MonoPages,
				&snapshot.ScanCount, &tonerJSON,
			)
			if err != nil {
				continue
			}

			if tonerJSON.Valid && tonerJSON.String != "" {
				json.Unmarshal([]byte(tonerJSON.String), &snapshot.TonerLevels)
			}

			// Filter by time range
			if (snapshot.Timestamp.Equal(since) || snapshot.Timestamp.After(since)) &&
				(snapshot.Timestamp.Equal(until) || snapshot.Timestamp.Before(until)) {
				snapshots = append(snapshots, snapshot)
			}
		}
	}

	// Query hourly aggregates (8-30 days ago)
	if needHourly {
		hourlyQuery := `
			SELECT id, serial, hour_start, page_count_avg, color_pages_avg, mono_pages_avg, scan_count_avg, toner_levels_avg
			FROM metrics_hourly
			WHERE serial = ?
			ORDER BY hour_start ASC
		`
		rows, err := s.db.QueryContext(ctx, hourlyQuery, serial)
		if err != nil {
			return nil, fmt.Errorf("failed to query hourly metrics: %w", err)
		}
		defer rows.Close()

		for rows.Next() {
			snapshot := &MetricsSnapshot{}
			var tonerJSON sql.NullString

			err := rows.Scan(
				&snapshot.ID, &snapshot.Serial, &snapshot.Timestamp,
				&snapshot.PageCount, &snapshot.ColorPages, &snapshot.MonoPages,
				&snapshot.ScanCount, &tonerJSON,
			)
			if err != nil {
				continue
			}

			if tonerJSON.Valid && tonerJSON.String != "" {
				json.Unmarshal([]byte(tonerJSON.String), &snapshot.TonerLevels)
			}

			if (snapshot.Timestamp.Equal(since) || snapshot.Timestamp.After(since)) &&
				(snapshot.Timestamp.Equal(until) || snapshot.Timestamp.Before(until)) {
				snapshots = append(snapshots, snapshot)
			}
		}
	}

	// Query daily aggregates (31-365 days ago)
	if needDaily {
		dailyQuery := `
			SELECT id, serial, day_start, page_count_avg, color_pages_avg, mono_pages_avg, scan_count_avg, toner_levels_avg
			FROM metrics_daily
			WHERE serial = ?
			ORDER BY day_start ASC
		`
		rows, err := s.db.QueryContext(ctx, dailyQuery, serial)
		if err != nil {
			return nil, fmt.Errorf("failed to query daily metrics: %w", err)
		}
		defer rows.Close()

		for rows.Next() {
			snapshot := &MetricsSnapshot{}
			var tonerJSON sql.NullString

			err := rows.Scan(
				&snapshot.ID, &snapshot.Serial, &snapshot.Timestamp,
				&snapshot.PageCount, &snapshot.ColorPages, &snapshot.MonoPages,
				&snapshot.ScanCount, &tonerJSON,
			)
			if err != nil {
				continue
			}

			if tonerJSON.Valid && tonerJSON.String != "" {
				json.Unmarshal([]byte(tonerJSON.String), &snapshot.TonerLevels)
			}

			if (snapshot.Timestamp.Equal(since) || snapshot.Timestamp.After(since)) &&
				(snapshot.Timestamp.Equal(until) || snapshot.Timestamp.Before(until)) {
				snapshots = append(snapshots, snapshot)
			}
		}
	}

	// Query monthly aggregates (>365 days ago)
	if needMonthly {
		monthlyQuery := `
			SELECT id, serial, month_start, page_count_avg, color_pages_avg, mono_pages_avg, scan_count_avg, toner_levels_avg
			FROM metrics_monthly
			WHERE serial = ?
			ORDER BY month_start ASC
		`
		rows, err := s.db.QueryContext(ctx, monthlyQuery, serial)
		if err != nil {
			return nil, fmt.Errorf("failed to query monthly metrics: %w", err)
		}
		defer rows.Close()

		for rows.Next() {
			snapshot := &MetricsSnapshot{}
			var tonerJSON sql.NullString

			err := rows.Scan(
				&snapshot.ID, &snapshot.Serial, &snapshot.Timestamp,
				&snapshot.PageCount, &snapshot.ColorPages, &snapshot.MonoPages,
				&snapshot.ScanCount, &tonerJSON,
			)
			if err != nil {
				continue
			}

			if tonerJSON.Valid && tonerJSON.String != "" {
				json.Unmarshal([]byte(tonerJSON.String), &snapshot.TonerLevels)
			}

			if (snapshot.Timestamp.Equal(since) || snapshot.Timestamp.After(since)) &&
				(snapshot.Timestamp.Equal(until) || snapshot.Timestamp.Before(until)) {
				snapshots = append(snapshots, snapshot)
			}
		}
	}

	return snapshots, nil
}
